# Disaster Response Pipeline Project

### Table of Contents
1. [Installation](#installation)
2. [Project Motivation](#motivation)
3. [File Descriptions](#files)
4. [Instructions](#instructions)   
5. [Licensing, Authors, and Acknowledgements](#licensing)

### Installation <a name="installation"></a>

All the dependencies to run the main app are collated in the requirements.txt file. I suggest that a virtual environment
is created using the file. The code should run with no issues using Python versions 3.*

The jupyter notebooks in the preprocessing folder were created using the standard Anaconda distribution of Python.

The dependencies are:

- click==7.1.2
- joblib==1.0.1
- langdetect==1.0.8
- nltk==3.5
- numpy==1.20.1
- pandas==1.2.3
- python-dateutil==2.8.1
- pytz==2021.1
- regex==2020.11.13
- scikit-learn==0.24.1
- scipy==1.6.1
- six==1.15.0
- SQLAlchemy==1.3.23
- threadpoolctl==2.1.0
- tqdm==4.58.0
- plotly~=4.14.3
- Flask~=1.1.2

### Project Motivation <a name="motivation"></a>

In this project, I will be using the dataset made available by [Figure 8](https://www.figure-eight.com/) to analyze
disaster data to build a model for an API that classifies disaster messages.

The project includes a web app where an emergency worker can input a new message and get classification results in 
several categories. The web app will also display visualizations of the dataset.

The advantage of such an app is to automate the process of analyzing disaster messages and using the output of the API
to notify the concerned organization. For example, a message reporting fire a locality, can be detected by the API which
can then forward this message onto the fire department. This will provide timely responses to messages of distress. 
A task which would otherwise require an human being to continuously monitor messages for possible incidents, is 
automated by the power of machine learning.

### File Descriptions <a name="files"></a>
    \app
        \templates
            go.html : main page of the web app
            master.html : results page of the web app
        run.py : the main python file to start the application
    \data
        disaster_categories.csv : contains categories information [from Figure 8](https://www.figure-eight.com/dataset/combined-disaster-response-data/)
        disaster_messages.csv : contains disaster messages and their labels [from Figure 8](https://www.figure-eight.com/dataset/combined-disaster-response-data/)
        DisasterResponse.db : The database file generated by the process_data.py script
        process_data.py : The script that loads, cleans and then stores the data as a sql database
    \images
        homepage.png : Screenshot of the app
        visualisation.png : Screenshot of the app
    \models
        classifier.pkl : The model generated by the train_classifier.py script in the pickle format
        custom_transformer.py : A file for the custom transformer -  meta data
        train_classifier.py : The script which trains and optimizes the machine learning model and stores it as a pickle file
    \preprocessing
        ETL Pipeline Preparation.ipynb : The preprocessing steps which lead to the design of the process_data.py script
        ML Pipldline Preparation.ipynb : The preprocessing steps which lead to the design of the train_classifier.py script
    LICENSE : The license file
    README.md : The readme file
    requirements.txt : The requirements file, use to create the virtual environment


### Instructions: <a name="instructions"></a>
1. Run the following commands in the project's root directory to set up your database and model.

    - To run ETL pipeline that cleans data and stores in database
        `python data/process_data.py data/disaster_messages.csv data/disaster_categories.csv data/DisasterResponse.db`
    - To run ML pipeline that trains classifier and saves
        `python models/train_classifier.py data/DisasterResponse.db models/classifier.pkl`

2. Run the following command in the app's directory to run your web app.
    `python run.py`

3. Go to http://0.0.0.0:3001/

## Licensing, Authors, Acknowledgements<a name="licensing"></a>
This project was done as part of the Udacity Data Scientist Nanodegree. The licensing information is detailed in the LICENSE file
